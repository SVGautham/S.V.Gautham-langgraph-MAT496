{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4cbf2458",
      "metadata": {
        "id": "4cbf2458"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-1/chain.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58238466-lesson-4-chain)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee55d3da-c53a-4c76-b46f-8e0d602e072e",
      "metadata": {
        "id": "ee55d3da-c53a-4c76-b46f-8e0d602e072e"
      },
      "source": [
        "# Chain\n",
        "\n",
        "## Review\n",
        "\n",
        "We built a simple graph with nodes, normal edges, and conditional edges.\n",
        "\n",
        "## Goals\n",
        "\n",
        "Now, let's build up to a simple chain that combines 4 [concepts](https://python.langchain.com/v0.2/docs/concepts/):\n",
        "\n",
        "* Using [chat messages](https://python.langchain.com/v0.2/docs/concepts/#messages) as our graph state\n",
        "* Using [chat models](https://python.langchain.com/v0.2/docs/concepts/#chat-models) in graph nodes\n",
        "* [Binding tools](https://python.langchain.com/v0.2/docs/concepts/#tools) to our chat model\n",
        "* [Executing tool calls](https://python.langchain.com/v0.2/docs/concepts/#functiontool-calling) in graph nodes\n",
        "\n",
        "![Screenshot 2024-08-21 at 9.24.03 AM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbab08dd607b08df5e1101_chain1.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a55e2e80-a718-4aaf-99b9-371157b34a4b",
      "metadata": {
        "id": "a55e2e80-a718-4aaf-99b9-371157b34a4b"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langchain_openai langchain_core langgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "OQ-c-dLjlKWI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQ-c-dLjlKWI",
        "outputId": "d4781148-4ebd-4bb6-b844-949af9a264c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-0.3.8-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (0.3.79)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (0.6.10)\n",
            "Collecting groq<1,>=0.30.0 (from langchain-groq)\n",
            "  Downloading groq-0.32.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.4.33)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (2.11.10)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.1.2)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.7.0,>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.6.4)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain-groq) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain-groq) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.11.0)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.4.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.30.0->langchain-groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain-groq) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain-groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain-groq) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2.5.0)\n",
            "Downloading langchain_groq-0.3.8-py3-none-any.whl (16 kB)\n",
            "Downloading groq-0.32.0-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq, langchain-groq\n",
            "Successfully installed groq-0.32.0 langchain-groq-0.3.8\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain-groq langchain-core langgraph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bMRApgFjnEy6",
      "metadata": {
        "id": "bMRApgFjnEy6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "hta0yWk7nFAU",
      "metadata": {
        "id": "hta0yWk7nFAU"
      },
      "source": [
        "## Messages\n",
        "\n",
        "Chat models can use [`messages`](https://python.langchain.com/v0.2/docs/concepts/#messages), which capture different roles within a conversation.\n",
        "\n",
        "LangChain supports various message types, including `HumanMessage`, `AIMessage`, `SystemMessage`, and `ToolMessage`.\n",
        "\n",
        "These represent a message from the user, from chat model, for the chat model to instruct behavior, and from a tool call.\n",
        "\n",
        "Let's create a list of messages.\n",
        "\n",
        "Each message can be supplied with a few things:\n",
        "\n",
        "* `content` - content of the message\n",
        "* `name` - optionally, a message author\n",
        "* `response_metadata` - optionally, a dict of metadata (e.g., often populated by model provider for `AIMessages`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PAthKB1qlPgG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAthKB1qlPgG",
        "outputId": "014ebc07-bd41-4cd8-b1b4-9bcdbbdabdc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: Model\n",
            "\n",
            "So you said you were researching ocean mammals?\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "Name: Lance\n",
            "\n",
            "Yes, that's right.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: Model\n",
            "\n",
            "Great, what would you like to learn about.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "Name: Lance\n",
            "\n",
            "I want to learn about the best place to see Orcas in the US.\n",
            "\n",
            "=== LLM Output ===\n",
            "### Top U.S. Spots for Seeing Orcas (Killer Whales)\n",
            "\n",
            "| Rank | Location | Why It’s Prime Orca‑watching | Best Time of Year | Typical Tour Options |\n",
            "|------|----------|-----------------------------|-------------------|----------------------|\n",
            "| **1** | **San Juan Islands, Washington** (especially Friday Harbor & Lime Kiln) | • Highest concentration of resident “Southern Resident” pods (J‑Pod, K‑Pod, L‑Pod).<br>• Calm, sheltered waters make sightings frequent and close. | **May – October** (peak: July‑September). | • Whale‑watch boats (30‑min to 3‑hr trips).<br>• Kayak tours (for the adventurous, 1‑2 hr). |\n",
            "| **2** | **Northern Washington Coast – Admiralty Inlet & San Juan Islands Passage** | • Transient (or “Bigg’s”) orcas follow salmon runs and marine mammals. | **April – September** (especially late summer). | • Larger “expedition” vessels (4‑6 hr) that also cover nearby islands. |\n",
            "| **3** | **Southwest Alaska – Glacier Bay & Icy Strait** | • Transient orcas hunt seals, sea lions, and Dall‑phased whales. <br>• Spectacular backdrop of glaciers. | **May – September** (peak: June‑August). | • Multi‑day cruise itineraries (e.g., Alaska cruise ships, specialty wildlife cruises). |\n",
            "| **4** | **Maui, Hawaii (West Maui Mountains & Lahaina Harbor)** | • Transient orcas appear seasonally while hunting dolphins and tuna. | **Winter & early spring (Nov‑Mar)** – occasional sightings. | • Small‑group boat tours from Lahaina or Ma‘alaea. |\n",
            "| **5** | **Monterey Bay, California** | • Occasional transient pods follow schools of fish and marine mammals. | **June – October** (when prey aggregations are high). | • Day‑trip whale‑watch boats from Monterey or Santa Cruz. |\n",
            "| **6** | **Cape Cod, Massachusetts (Stellwagen Bank National Marine Sanctuary)** | • Rare transient sightings; more common in the North Atlantic, but occasional “Pacific‑type” orcas have been reported. | **Late summer (July‑August)**. | • Guided whale‑watch tours from Provincetown or Barnstable. |\n",
            "\n",
            "---\n",
            "\n",
            "## Quick Guide to Planning Your Orca‑watch Trip\n",
            "\n",
            "### 1. Pick the Right Season\n",
            "- **Resident orcas** (San Juan Islands) are most reliably seen **July‑September** when they feed on Chinook salmon.\n",
            "- **Transient orcas** (Alaska, Monterey Bay) follow seals, sea lions, and other whales; they’re most active **late spring‑summer**.\n",
            "- **Hawaiian sightings** are seasonal and less predictable; plan for a **flexible itinerary**.\n",
            "\n",
            "### 2. Choose a Reputable Tour Operator\n",
            "- Look for operators with **U.S. Coast Guard‑certified captains** and **small‑group vessels** (max 12‑20 passengers) for better maneuverability.\n",
            "- Check **review sites** (TripAdvisor, Google) and confirm they follow **responsible wildlife‑watching guidelines** (e.g., maintaining a minimum distance of 100 m from whales).\n",
            "\n",
            "### 3. Gear & Preparation\n",
            "| Item | Why It Helps |\n",
            "|------|--------------|\n",
            "| **Water‑proof binoculars** (8×42) | Spot distant blows and spouts. |\n",
            "| **Layered clothing** | Early mornings on the water can be chilly, even in summer. |\n",
            "| **Sun protection** (hat, sunscreen) | Sun reflects off the water. |\n",
            "| **Camera with telephoto lens (≥200 mm)** | Capture close‑up action without disturbing the whales. |\n",
            "| **Motion‑sickness medication** (if needed) | Boats can be bumpy in choppy conditions. |\n",
            "\n",
            "### 4. Etiquette & Conservation\n",
            "- **Maintain distance**: Never approach closer than the legally mandated 100 m (330 ft) for resident orcas in Washington.\n",
            "- **No feeding or loud noises**: This can stress the animals.\n",
            "- **Leave no trace**: Pack out all trash and avoid single‑use plastics.\n",
            "- **Support local conservation**: Many tour operators contribute a portion of proceeds to Orca‑watch or marine‑protected‑area programs.\n",
            "\n",
            "### 5. Bonus Experiences\n",
            "- **Orca‑watch festivals**: The **San Juan Islands Orca Festival** (early July) includes talks, boat tours, and a “Orca‑watching challenge.”\n",
            "- **Shore‑based viewing**: At **San Juan Island’s Lime Kiln** you can watch orcas from the shore while waiting for a boat to pick you up.\n",
            "- **Combine with other wildlife**: In Alaska, you can often see humpback whales, sea otters, and sea eagles on the same trip.\n",
            "\n",
            "---\n",
            "\n",
            "## Sample 3‑Day Itinerary – San Juan Islands (Most Reliable)\n",
            "\n",
            "| Day | Morning | Afternoon | Evening |\n",
            "|-----|----------|-----------|----------|\n",
            "| **1** | Fly into **Seattle** → drive to **Anacortes** (≈1 hr).<br>Take the **San Juan Express** ferry to **Friday Harbor**. | Check‑in, explore Friday Harbor, optional **Lime Kiln** walk. | Dinner at a local seafood restaurant; early night (whale‑watch starts early). |\n",
            "| **2** | **7:30 am** – Depart on a 3‑hr resident‑orca tour (most operators launch around sunrise).<br>Spot J‑Pod or K‑Pod feeding on salmon. | Return to harbor; **Lunch** at a waterfront café.<br>Optional **Kayak** excursion for a different perspective. | Attend a **talk** by a marine biologist (many lodges host free talks). |\n",
            "| **3** | **8:00 am** – Half‑day “transient‑orca” tour (focus on seals, sea lions). | Return, pack, ferry back to **Anacortes**, drive to **Seattle**. | Fly home or extend to explore Seattle’s waterfront. |\n",
            "\n",
            "*Tip:* Book your whale‑watch tour **at least 2‑3 weeks** in advance during peak season; many operators sell out quickly.\n",
            "\n",
            "---\n",
            "\n",
            "### Bottom Line\n",
            "- **Best overall odds**: **San Juan Islands, Washington** (resident pods, frequent sightings, well‑established tours).\n",
            "- **Most dramatic scenery**: **Glacier Bay, Alaska** (orcas alongside towering glaciers).\n",
            "- **If you’re on a West Coast road‑trip**: Combine a day in **Monterey Bay** with a longer Alaskan cruise for a “two‑stage” orca‑watch experience.\n",
            "\n",
            "Happy sailing, and may you get a spectacular breach! 🌊🐋\n",
            "\n",
            "=== Tool Call Info ===\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_groq import ChatGroq\n",
        "from typing_extensions import TypedDict\n",
        "from typing import Annotated\n",
        "from langgraph.graph import MessagesState, StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "import os\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1. Setup Groq API Key\n",
        "# ------------------------------------------------------------------\n",
        "os.environ[\"GROQ_API_KEY\"] = \"\"\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2. Create messages\n",
        "# ------------------------------------------------------------------\n",
        "messages = [\n",
        "    AIMessage(content=\"So you said you were researching ocean mammals?\", name=\"Model\"),\n",
        "    HumanMessage(content=\"Yes, that's right.\", name=\"Lance\"),\n",
        "    AIMessage(content=\"Great, what would you like to learn about.\", name=\"Model\"),\n",
        "    HumanMessage(content=\"I want to learn about the best place to see Orcas in the US.\", name=\"Lance\"),\n",
        "]\n",
        "\n",
        "for m in messages:\n",
        "    m.pretty_print()\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 3. Initialize Groq model\n",
        "# ------------------------------------------------------------------\n",
        "# Choose a Groq-supported model (e.g., \"llama3-8b-8192\" or \"mixtral-8x7b-32768\")\n",
        "llm = ChatGroq(model=\"openai/gpt-oss-120b\")\n",
        "\n",
        "# Test basic message invocation\n",
        "result = llm.invoke(messages)\n",
        "print(\"\\n=== LLM Output ===\")\n",
        "print(result.content)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 4. Define a tool\n",
        "# ------------------------------------------------------------------\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply a and b.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "# Bind tool to model\n",
        "llm_with_tools = llm.bind_tools([multiply])\n",
        "\n",
        "# Example: asking the model to multiply\n",
        "tool_call = llm_with_tools.invoke([HumanMessage(content=\"What is 2 multiplied by 3?\")])\n",
        "print(\"\\n=== Tool Call Info ===\")\n",
        "print(tool_call.tool_calls)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yu9VvbM9mdCk",
      "metadata": {
        "id": "yu9VvbM9mdCk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "FgOxuyGvmdKh",
      "metadata": {
        "id": "FgOxuyGvmdKh"
      },
      "source": [
        "## Using messages as state\n",
        "\n",
        "With these foundations in place, we can now use [`messages`](https://python.langchain.com/v0.2/docs/concepts/#messages) in our graph state.\n",
        "\n",
        "Let's define our state, `MessagesState`, as a `TypedDict` with a single key: `messages`.\n",
        "\n",
        "`messages` is simply a list of messages, as we defined above (e.g., `HumanMessage`, etc)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_3skX3_QnM9q",
      "metadata": {
        "id": "_3skX3_QnM9q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "tKmkpi34nNUk",
      "metadata": {
        "id": "tKmkpi34nNUk"
      },
      "source": [
        "## Tools\n",
        "\n",
        "Tools are useful whenever you want a model to interact with external systems.\n",
        "\n",
        "External systems (e.g., APIs) often require a particular input schema or payload, rather than natural language.\n",
        "\n",
        "When we bind an API, for example, as a tool we given the model awareness of the required input schema.\n",
        "\n",
        "The model will choose to call a tool based upon the natural language input from the user.\n",
        "\n",
        "And, it will return an output that adheres to the tool's schema.\n",
        "\n",
        "[Many LLM providers support tool calling](https://python.langchain.com/v0.1/docs/integrations/chat/) and [tool calling interface](https://blog.langchain.dev/improving-core-tool-interfaces-and-docs-in-langchain/) in LangChain is simple.\n",
        "\n",
        "You can simply pass any Python `function` into `ChatModel.bind_tools(function)`.\n",
        "\n",
        "![Screenshot 2024-08-19 at 7.46.28 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbab08dc1c17a7a57f9960_chain2.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "pRjALXiFl1ij",
      "metadata": {
        "id": "pRjALXiFl1ij"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------\n",
        "# 5. Define graph state and reducer\n",
        "# ------------------------------------------------------------------\n",
        "class MyMessagesState(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 6. Define a node (function run by the graph)\n",
        "# ------------------------------------------------------------------\n",
        "def tool_calling_llm(state: MyMessagesState):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 7. Build and compile the LangGraph\n",
        "# ------------------------------------------------------------------\n",
        "builder = StateGraph(MyMessagesState)\n",
        "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
        "builder.add_edge(START, \"tool_calling_llm\")\n",
        "builder.add_edge(\"tool_calling_llm\", END)\n",
        "graph = builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9_gj5c5nmjjM",
      "metadata": {
        "id": "9_gj5c5nmjjM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "Z4qtCaa_mju4",
      "metadata": {
        "id": "Z4qtCaa_mju4"
      },
      "source": [
        "## Our graph\n",
        "\n",
        "Now, lets use `MessagesState` with a graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7mBomgz4llmA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mBomgz4llmA",
        "outputId": "5cfe0213-15c9-4b0b-9abf-36ec77fb20de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Conversation 1 ===\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Hello!\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello! How can I assist you today?\n",
            "\n",
            "=== Conversation 2 (Tool Example) ===\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Multiply 2 and 3!\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  multiply (fc_4f9c46f7-9a64-4606-b62b-5cf557efc110)\n",
            " Call ID: fc_4f9c46f7-9a64-4606-b62b-5cf557efc110\n",
            "  Args:\n",
            "    a: 2\n",
            "    b: 3\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------------\n",
        "# 8. Test the graph\n",
        "# ------------------------------------------------------------------\n",
        "print(\"\\n=== Conversation 1 ===\")\n",
        "messages = graph.invoke({\"messages\": [HumanMessage(content=\"Hello!\")]})\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()\n",
        "\n",
        "print(\"\\n=== Conversation 2 (Tool Example) ===\")\n",
        "messages = graph.invoke({\"messages\": [HumanMessage(content=\"Multiply 2 and 3!\") ]})\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ln5neAnDnT2e",
      "metadata": {
        "id": "ln5neAnDnT2e"
      },
      "source": [
        "#EXAMPLE SOLVED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hDHRJA9unTpO",
      "metadata": {
        "id": "hDHRJA9unTpO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "jquh2S1Yl-kN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jquh2S1Yl-kN",
        "outputId": "d5d8c2d0-fc47-4410-add7-fbeed35dced0"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Example 1: Basic Conversation ===\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Hello, how are you today?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello! I'm doing great, thank you for asking. How can I assist you today?\n",
            "\n",
            "=== Example 2: Asking for a Math Operation ===\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Can you multiply 7 and 8 for me?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  multiply (fc_db0c54f3-f5d5-48c4-a4eb-bb7a53359883)\n",
            " Call ID: fc_db0c54f3-f5d5-48c4-a4eb-bb7a53359883\n",
            "  Args:\n",
            "    a: 7\n",
            "    b: 8\n",
            "\n",
            "=== Example 3: Using Area Tool ===\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What’s the area of a circle with radius 5?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  get_area_of_circle (fc_e2aa06c7-54ee-4da8-9607-afb3d442fd19)\n",
            " Call ID: fc_e2aa06c7-54ee-4da8-9607-afb3d442fd19\n",
            "  Args:\n",
            "    radius: 5\n",
            "\n",
            "=== Example 4: Asking for Current Time ===\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What is the current time right now?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  current_time (fc_982a0803-274d-47cc-980e-4829efb0b3e7)\n",
            " Call ID: fc_982a0803-274d-47cc-980e-4829efb0b3e7\n",
            "  Args:\n",
            "    : {}\n",
            "\n",
            "=== Example 5: Context-Aware Conversation ===\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Hi there!\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello! What can I help you with today?\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "I’m learning about LangGraph. Can you give me a short summary?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "**LangGraph – a quick rundown**\n",
            "\n",
            "LangGraph is a lightweight framework for building **state‑driven, multi‑step workflows** (or “graphs”) that orchestrate language‑model calls, tool usage, and custom Python logic. It lets you treat an LLM‑powered application as a **graph of nodes** (steps) connected by edges, with a shared mutable **state** that flows through the graph.\n",
            "\n",
            "| Aspect | What it is | Why it matters |\n",
            "|--------|------------|----------------|\n",
            "| **Nodes** | Individual units of work – can be an LLM call, a tool/function, or any Python function. | Encapsulates reusable logic; each node can read/write the graph’s state. |\n",
            "| **Edges** | Directed connections that decide the next node based on the current state (often via a “router” node). | Enables conditional branching, loops, and parallelism. |\n",
            "| **State** | A mutable dictionary (or pydantic model) that persists across nodes. | Gives the graph memory; you can store intermediate results, user context, or tool outputs. |\n",
            "| **Loops & Recursion** | Nodes can point back to earlier nodes, allowing retry, refinement, or multi‑turn conversations. | Handles tasks that need iteration (e.g., re‑asking a question, tool‑calling loops). |\n",
            "| **Tool Integration** | You can register arbitrary Python functions (or external APIs) as tools that the LLM can invoke. | Bridges the LLM with real‑world actions (search, DB queries, calculations, etc.). |\n",
            "| **Agent‑like behavior** | By combining an LLM “router” node with tool nodes, you get an autonomous agent that decides which tool to call next. | Makes building “self‑directed” assistants simpler than hand‑coding the control flow. |\n",
            "| **Composable** | Graphs can be nested or combined; a node can itself be another LangGraph. | Promotes modular design and reuse across projects. |\n",
            "| **Streaming & Async** | Supports async nodes and streaming token output from LLM calls. | Enables responsive UIs and efficient handling of long‑running tasks. |\n",
            "| **Observability** | Built‑in tracing hooks (via LangChain’s `Callbacks`) to log state transitions, timings, and token usage. | Helpful for debugging and monitoring production agents. |\n",
            "\n",
            "### Typical workflow\n",
            "\n",
            "1. **Define a state schema** (e.g., `class MyState(BaseModel): user_input: str; answer: str | None`).\n",
            "2. **Create nodes**:  \n",
            "   * `router` – an LLM prompt that decides the next step.  \n",
            "   * `search_tool` – a Python function that hits a search API.  \n",
            "   * `generate_answer` – LLM call that uses `search_results` from state.\n",
            "3. **Wire edges**: `router → search_tool` if the LLM says “search”; otherwise `router → generate_answer`.\n",
            "4. **Instantiate the graph** and call `graph.invoke({\"user_input\": \"...\"} )`.\n",
            "5. The graph runs, updating the state at each step, until it reaches a terminal node (often `END`).\n",
            "\n",
            "### Why use LangGraph?\n",
            "\n",
            "- **Declarative flow**: You describe *what* should happen, not *how* to orchestrate async calls.\n",
            "- **Reusable components**: Nodes are plug‑and‑play; you can swap an LLM model or a tool without rewriting the whole pipeline.\n",
            "- **Built‑in looping**: Complex iterative patterns (e.g., “keep asking the user until clarification”) become trivial.\n",
            "- **Alignment with LangChain**: If you already use LangChain for LLM wrappers, LangGraph feels like a natural extension for control flow.\n",
            "\n",
            "### Simple code sketch\n",
            "\n",
            "```python\n",
            "from langgraph.graph import StateGraph, END\n",
            "from pydantic import BaseModel\n",
            "from langchain.llms import OpenAI\n",
            "\n",
            "class MyState(BaseModel):\n",
            "    user_input: str\n",
            "    search_results: str | None = None\n",
            "    answer: str | None = None\n",
            "\n",
            "def router(state: MyState):\n",
            "    # LLM decides what to do\n",
            "    decision = llm.predict(\n",
            "        \"User asked: {user_input}. Should we SEARCH or ANSWER directly?\"\n",
            "    )\n",
            "    return \"search\" if \"search\" in decision.lower() else \"answer\"\n",
            "\n",
            "def search_tool(state: MyState):\n",
            "    # pretend we call an external API\n",
            "    state.search_results = fake_search(state.user_input)\n",
            "    return state\n",
            "\n",
            "def answer(state: MyState):\n",
            "    prompt = f\"User: {state.user_input}\\n\"\n",
            "    if state.search_results:\n",
            "        prompt += f\"Relevant info: {state.search_results}\\n\"\n",
            "    state.answer = llm.predict(prompt)\n",
            "    return state\n",
            "\n",
            "graph = StateGraph(MyState)\n",
            "graph.add_node(\"router\", router)\n",
            "graph.add_node(\"search\", search_tool)\n",
            "graph.add_node(\"answer\", answer)\n",
            "\n",
            "graph.set_entry_point(\"router\")\n",
            "graph.add_conditional_edges(\n",
            "    \"router\",\n",
            "    lambda s: \"search\" if \"search\" in s else \"answer\",\n",
            "    {\"search\": \"search\", \"answer\": \"answer\"},\n",
            ")\n",
            "graph.add_edge(\"search\", \"answer\")\n",
            "graph.add_edge(\"answer\", END)\n",
            "\n",
            "app = graph.compile()\n",
            "result = app.invoke({\"user_input\": \"What’s the latest news on AI?\"})\n",
            "print(result[\"answer\"])\n",
            "```\n",
            "\n",
            "That snippet shows the core ideas: a state model, nodes, conditional routing, and a terminal `END`.\n",
            "\n",
            "---\n",
            "\n",
            "**Bottom line:** LangGraph gives you a clean, graph‑oriented way to orchestrate LLM calls, tools, and Python logic with persistent state, loops, and branching—all while staying compatible with the broader LangChain ecosystem.\n",
            "\n",
            "🎉 Done! You have now demonstrated:\n",
            " - Groq LLM + LangGraph integration\n",
            " - Multiple bound tools\n",
            " - Stateful message-based conversation\n",
            " - Graph visualization via Mermaid diagram\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "from IPython.display import Image, display\n",
        "from datetime import datetime\n",
        "# ------------------------------\n",
        "# 🧰 Define Tools\n",
        "# ------------------------------\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply two numbers.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "def get_area_of_circle(radius: float) -> float:\n",
        "    \"\"\"Calculate area of a circle given its radius.\"\"\"\n",
        "    return round(math.pi * radius ** 2, 2)\n",
        "\n",
        "def current_time() -> str:\n",
        "    \"\"\"Return the current date and time.\"\"\"\n",
        "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "# Bind all tools to the model\n",
        "llm_with_tools = llm.bind_tools([multiply, get_area_of_circle, current_time])\n",
        "\n",
        "# ------------------------------\n",
        "# 🧱 Define Graph State\n",
        "# ------------------------------\n",
        "class MessagesState(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "# ------------------------------\n",
        "# ⚙️ Define Node Function\n",
        "# ------------------------------\n",
        "def tool_calling_llm(state: MessagesState):\n",
        "    \"\"\"Process messages and allow the model to call bound tools.\"\"\"\n",
        "    response = llm_with_tools.invoke(state[\"messages\"])\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "# ------------------------------\n",
        "# 🕸️ Build the Graph\n",
        "# ------------------------------\n",
        "builder = StateGraph(MessagesState)\n",
        "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
        "builder.add_edge(START, \"tool_calling_llm\")\n",
        "builder.add_edge(\"tool_calling_llm\", END)\n",
        "graph = builder.compile()\n",
        "\n",
        "# ------------------------------\n",
        "# 🖼️ Visualize the Graph (Flow Image)\n",
        "# ------------------------------\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "\n",
        "# ------------------------------\n",
        "# 🧑‍💬 Example 1 — Basic Chat\n",
        "# ------------------------------\n",
        "print(\"\\n=== Example 1: Basic Conversation ===\")\n",
        "result = graph.invoke({\"messages\": [HumanMessage(content=\"Hello, how are you today?\")]})\n",
        "for msg in result[\"messages\"]:\n",
        "    msg.pretty_print()\n",
        "\n",
        "# ------------------------------\n",
        "# 🔢 Example 2 — Using a Tool (Math)\n",
        "# ------------------------------\n",
        "print(\"\\n=== Example 2: Asking for a Math Operation ===\")\n",
        "result = graph.invoke({\"messages\": [HumanMessage(content=\"Can you multiply 7 and 8 for me?\")]})\n",
        "for msg in result[\"messages\"]:\n",
        "    msg.pretty_print()\n",
        "\n",
        "# ------------------------------\n",
        "# ⚪ Example 3 — Another Tool (Area Calculation)\n",
        "# ------------------------------\n",
        "print(\"\\n=== Example 3: Using Area Tool ===\")\n",
        "result = graph.invoke({\"messages\": [HumanMessage(content=\"What’s the area of a circle with radius 5?\")]})\n",
        "for msg in result[\"messages\"]:\n",
        "    msg.pretty_print()\n",
        "\n",
        "# ------------------------------\n",
        "# 🕓 Example 4 — Time Query Tool\n",
        "# ------------------------------\n",
        "print(\"\\n=== Example 4: Asking for Current Time ===\")\n",
        "result = graph.invoke({\"messages\": [HumanMessage(content=\"What is the current time right now?\")]})\n",
        "for msg in result[\"messages\"]:\n",
        "    msg.pretty_print()\n",
        "\n",
        "# ------------------------------\n",
        "# 💬 Example 5 — Context Carryover\n",
        "# ------------------------------\n",
        "print(\"\\n=== Example 5: Context-Aware Conversation ===\")\n",
        "conversation = [\n",
        "    HumanMessage(content=\"Hi there!\"),\n",
        "    AIMessage(content=\"Hello! What can I help you with today?\"),\n",
        "    HumanMessage(content=\"I’m learning about LangGraph. Can you give me a short summary?\"),\n",
        "]\n",
        "result = graph.invoke({\"messages\": conversation})\n",
        "for msg in result[\"messages\"]:\n",
        "    msg.pretty_print()\n",
        "\n",
        "print(\"\\n🎉 Done! You have now demonstrated:\")\n",
        "print(\" - Groq LLM + LangGraph integration\")\n",
        "print(\" - Multiple bound tools\")\n",
        "print(\" - Stateful message-based conversation\")\n",
        "print(\" - Graph visualization via Mermaid diagram\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "311fbae3",
      "metadata": {
        "id": "311fbae3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "lc-academy-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
